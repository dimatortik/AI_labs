{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14183377,"sourceType":"datasetVersion","datasetId":9031572},{"sourceId":14202040,"sourceType":"datasetVersion","datasetId":9057579},{"sourceId":455195,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":369173,"modelId":390046}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport glob\nimport shutil\nimport random\nimport time\nfrom dataclasses import dataclass\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.utils.class_weight import compute_class_weight\n\nimport cv2\nfrom PIL import Image\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n\n@dataclass\nclass CFG:\n    DATASET_DIR: str = \"/kaggle/input/lab6-spotify-dataset/dataset\"\n\n    WORK_RAW_DIR: str = \"/kaggle/working/raw_dataset\"\n    WORK_AUG_DIR: str = \"/kaggle/working/augmented_dataset\"\n    WORK_SPLIT_DIR: str = \"/kaggle/working/split_dataset\"\n\n    TARGET_SIZE: tuple = (299, 299)\n    BATCH_SIZE: int = 16\n\n    SPLIT_RATIOS: tuple = (0.7, 0.15, 0.15)\n\n    OFFLINE_AUG_MAX_PER_IMAGE: int = 5\n    OFFLINE_AUG_FILL_MODE: str = \"nearest\"\n\n    ONLINE_AUG: bool = True\n\n    EPOCHS: int = 30\n    LR_FROZEN: float = 1e-3\n    LR_FINE: float = 1e-4\n    PATIENCE: int = 6\n    FROZEN_EPOCHS: int = 5\n\n    IMAGENET_WEIGHTS_PATH: str = (\n        \"/kaggle/input/xception_weights_tf_dim_ordering_tf_kernels_notop/\"\n        \"tensorflow2/default/1/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n    )\n\n    VIDEO_PATH: str = \"/kaggle/input/spoti-detection/spotify-video-for-detection.mp4\"\n    VIDEO_BATCH: int = 32\n    VIDEO_SKIP_FRAMES: int = 3\n    VIDEO_THRESHOLD: float = 0.5\n    SMOOTH_WINDOW: int = 7\n    MIN_INTERVAL_SEC: float = 0.3\n\n\nCFG = CFG()\n\ndef xception_preprocess(x):\n    return (x / 127.5) - 1.0\n\ndef find_binary_dataset_dir():\n    candidates = []\n    for p in glob.glob(\"/kaggle/input/**\", recursive=True):\n        if os.path.isdir(p) and os.path.isdir(os.path.join(p, \"positive\")) and os.path.isdir(os.path.join(p, \"negative\")):\n            candidates.append(p)\n    candidates.sort(key=lambda x: (0 if os.path.basename(x).lower() == \"dataset\" else 1, len(x)))\n    return candidates[0] if candidates else None\n\ndef clean_corrupted_images(root_dir):\n    exts = (\".jpg\", \".jpeg\", \".png\", \".webp\", \".bmp\", \".gif\", \".tiff\")\n    removed = 0\n    total = 0\n    for sub in [\"positive\", \"negative\"]:\n        d = os.path.join(root_dir, sub)\n        if not os.path.isdir(d):\n            continue\n        for fn in os.listdir(d):\n            if not fn.lower().endswith(exts):\n                continue\n            total += 1\n            fp = os.path.join(d, fn)\n            try:\n                with Image.open(fp) as img:\n                    img.verify()\n                with Image.open(fp) as img:\n                    img = img.convert(\"RGB\")\n                    img.save(fp)\n            except Exception:\n                try:\n                    os.remove(fp)\n                    removed += 1\n                except Exception:\n                    pass\n    return total, removed\n\n# -------------------------\n# Offline augmentation (balance classes BEFORE training)\n# -------------------------\ndef offline_augment_to_balance(src_dir, out_dir, target_size=(299, 299), max_per_image=5):\n    if os.path.exists(out_dir):\n        shutil.rmtree(out_dir)\n    os.makedirs(os.path.join(out_dir, \"positive\"), exist_ok=True)\n    os.makedirs(os.path.join(out_dir, \"negative\"), exist_ok=True)\n\n    # Copy originals\n    for cls in [\"positive\", \"negative\"]:\n        shutil.copytree(os.path.join(src_dir, cls), os.path.join(out_dir, cls), dirs_exist_ok=True)\n\n    def count_images(cls):\n        exts = (\".jpg\", \".jpeg\", \".png\", \".webp\", \".bmp\")\n        d = os.path.join(out_dir, cls)\n        return len([f for f in os.listdir(d) if f.lower().endswith(exts)])\n\n    pos_n = count_images(\"positive\")\n    neg_n = count_images(\"negative\")\n    print(f\"[INFO] Before offline augmentation: pos={pos_n}, neg={neg_n}\")\n\n    if pos_n == neg_n:\n        print(\"[INFO] Classes already balanced. Skipping offline augmentation.\")\n        return\n\n    minority = \"positive\" if pos_n < neg_n else \"negative\"\n    need = abs(pos_n - neg_n)\n    print(f\"[INFO] Offline augmenting minority='{minority}' to add ~{need} images\")\n\n    aug_gen = ImageDataGenerator(\n        rotation_range=20,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.15,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        brightness_range=[0.8, 1.2],\n        fill_mode=CFG.OFFLINE_AUG_FILL_MODE\n    )\n\n    exts = (\".jpg\", \".jpeg\", \".png\", \".webp\", \".bmp\")\n    in_dir = os.path.join(out_dir, minority)\n    src_files = [f for f in os.listdir(in_dir) if f.lower().endswith(exts)]\n    if not src_files:\n        raise RuntimeError(f\"No images found in class '{minority}' for offline augmentation\")\n\n    made = 0\n    i = 0\n    while made < need:\n        fn = src_files[i % len(src_files)]\n        i += 1\n\n        img_path = os.path.join(in_dir, fn)\n        try:\n            img = load_img(img_path, target_size=target_size)\n            x = img_to_array(img)\n            x = np.expand_dims(x, 0)\n        except Exception:\n            continue\n\n        base = os.path.splitext(fn)[0]\n        j = 0\n        for batch in aug_gen.flow(x, batch_size=1):\n            out_name = f\"{base}_aug_{made:06d}.jpg\"\n            out_path = os.path.join(in_dir, out_name)\n            tf.keras.preprocessing.image.array_to_img(batch[0]).save(out_path)\n            made += 1\n            j += 1\n            if made >= need or j >= max_per_image:\n                break\n\n    pos_n2 = count_images(\"positive\")\n    neg_n2 = count_images(\"negative\")\n    print(f\"[INFO] After offline augmentation: pos={pos_n2}, neg={neg_n2}\")\n\n# -------------------------\n# Build split dataset: train/val/test\n# -------------------------\ndef make_splits(src_dir, out_dir, ratios=(0.7, 0.15, 0.15), seed=42):\n    if os.path.exists(out_dir):\n        shutil.rmtree(out_dir)\n\n    for split in [\"train\", \"val\", \"test\"]:\n        for cls in [\"positive\", \"negative\"]:\n            os.makedirs(os.path.join(out_dir, split, cls), exist_ok=True)\n\n    exts = (\".jpg\", \".jpeg\", \".png\", \".webp\", \".bmp\")\n    rng = random.Random(seed)\n\n    for cls in [\"positive\", \"negative\"]:\n        files = [f for f in os.listdir(os.path.join(src_dir, cls)) if f.lower().endswith(exts)]\n        rng.shuffle(files)\n\n        n = len(files)\n        n_train = int(n * ratios[0])\n        n_val = int(n * ratios[1])\n\n        train_files = files[:n_train]\n        val_files = files[n_train:n_train + n_val]\n        test_files = files[n_train + n_val:]\n\n        for f in train_files:\n            shutil.copy2(os.path.join(src_dir, cls, f), os.path.join(out_dir, \"train\", cls, f))\n        for f in val_files:\n            shutil.copy2(os.path.join(src_dir, cls, f), os.path.join(out_dir, \"val\", cls, f))\n        for f in test_files:\n            shutil.copy2(os.path.join(src_dir, cls, f), os.path.join(out_dir, \"test\", cls, f))\n\n        print(f\"[INFO] Split '{cls}': train={len(train_files)}, val={len(val_files)}, test={len(test_files)}\")\n\n# -------------------------\n# Xception backbone\n# -------------------------\ndef build_xception_backbone_exact(input_shape=(299, 299, 3)):\n    img_input = layers.Input(shape=input_shape, name=\"input\")\n\n    # block1\n    x = layers.Conv2D(32, (3, 3), strides=(2, 2), use_bias=False, padding=\"valid\", name=\"block1_conv1\")(img_input)\n    x = layers.BatchNormalization(name=\"block1_conv1_bn\")(x)\n    x = layers.Activation(\"relu\", name=\"block1_conv1_act\")(x)\n\n    x = layers.Conv2D(64, (3, 3), use_bias=False, padding=\"valid\", name=\"block1_conv2\")(x)\n    x = layers.BatchNormalization(name=\"block1_conv2_bn\")(x)\n    x = layers.Activation(\"relu\", name=\"block1_conv2_act\")(x)\n\n    # block2\n    residual = layers.Conv2D(128, (1, 1), strides=(2, 2), padding=\"same\", use_bias=False, name=\"block2_res_conv\")(x)\n    residual = layers.BatchNormalization(name=\"block2_res_conv_bn\")(residual)\n\n    x = layers.SeparableConv2D(128, (3, 3), padding=\"same\", use_bias=False, name=\"block2_sepconv1\")(x)\n    x = layers.BatchNormalization(name=\"block2_sepconv1_bn\")(x)\n    x = layers.Activation(\"relu\", name=\"block2_sepconv1_act\")(x)\n\n    x = layers.SeparableConv2D(128, (3, 3), padding=\"same\", use_bias=False, name=\"block2_sepconv2\")(x)\n    x = layers.BatchNormalization(name=\"block2_sepconv2_bn\")(x)\n\n    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\", name=\"block2_pool\")(x)\n    x = layers.Add(name=\"block2_add\")([x, residual])\n\n    # block3\n    residual = layers.Conv2D(256, (1, 1), strides=(2, 2), padding=\"same\", use_bias=False, name=\"block3_res_conv\")(x)\n    residual = layers.BatchNormalization(name=\"block3_res_conv_bn\")(residual)\n\n    x = layers.Activation(\"relu\", name=\"block3_sepconv1_act\")(x)\n    x = layers.SeparableConv2D(256, (3, 3), padding=\"same\", use_bias=False, name=\"block3_sepconv1\")(x)\n    x = layers.BatchNormalization(name=\"block3_sepconv1_bn\")(x)\n\n    x = layers.Activation(\"relu\", name=\"block3_sepconv2_act\")(x)\n    x = layers.SeparableConv2D(256, (3, 3), padding=\"same\", use_bias=False, name=\"block3_sepconv2\")(x)\n    x = layers.BatchNormalization(name=\"block3_sepconv2_bn\")(x)\n\n    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\", name=\"block3_pool\")(x)\n    x = layers.Add(name=\"block3_add\")([x, residual])\n\n    # block4\n    residual = layers.Conv2D(728, (1, 1), strides=(2, 2), padding=\"same\", use_bias=False, name=\"block4_res_conv\")(x)\n    residual = layers.BatchNormalization(name=\"block4_res_conv_bn\")(residual)\n\n    x = layers.Activation(\"relu\", name=\"block4_sepconv1_act\")(x)\n    x = layers.SeparableConv2D(728, (3, 3), padding=\"same\", use_bias=False, name=\"block4_sepconv1\")(x)\n    x = layers.BatchNormalization(name=\"block4_sepconv1_bn\")(x)\n\n    x = layers.Activation(\"relu\", name=\"block4_sepconv2_act\")(x)\n    x = layers.SeparableConv2D(728, (3, 3), padding=\"same\", use_bias=False, name=\"block4_sepconv2\")(x)\n    x = layers.BatchNormalization(name=\"block4_sepconv2_bn\")(x)\n\n    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\", name=\"block4_pool\")(x)\n    x = layers.Add(name=\"block4_add\")([x, residual])\n\n    # middle flow blocks 5..12 (8 blocks)\n    for i in range(8):\n        block_id = 5 + i\n        prefix = f\"block{block_id}\"\n        residual = x\n\n        x = layers.Activation(\"relu\", name=f\"{prefix}_sepconv1_act\")(x)\n        x = layers.SeparableConv2D(728, (3, 3), padding=\"same\", use_bias=False, name=f\"{prefix}_sepconv1\")(x)\n        x = layers.BatchNormalization(name=f\"{prefix}_sepconv1_bn\")(x)\n\n        x = layers.Activation(\"relu\", name=f\"{prefix}_sepconv2_act\")(x)\n        x = layers.SeparableConv2D(728, (3, 3), padding=\"same\", use_bias=False, name=f\"{prefix}_sepconv2\")(x)\n        x = layers.BatchNormalization(name=f\"{prefix}_sepconv2_bn\")(x)\n\n        x = layers.Activation(\"relu\", name=f\"{prefix}_sepconv3_act\")(x)\n        x = layers.SeparableConv2D(728, (3, 3), padding=\"same\", use_bias=False, name=f\"{prefix}_sepconv3\")(x)\n        x = layers.BatchNormalization(name=f\"{prefix}_sepconv3_bn\")(x)\n\n        x = layers.Add(name=f\"{prefix}_add\")([x, residual])\n\n    # exit flow: block13\n    residual = layers.Conv2D(1024, (1, 1), strides=(2, 2), padding=\"same\", use_bias=False, name=\"block13_res_conv\")(x)\n    residual = layers.BatchNormalization(name=\"block13_res_conv_bn\")(residual)\n\n    x = layers.Activation(\"relu\", name=\"block13_sepconv1_act\")(x)\n    x = layers.SeparableConv2D(728, (3, 3), padding=\"same\", use_bias=False, name=\"block13_sepconv1\")(x)\n    x = layers.BatchNormalization(name=\"block13_sepconv1_bn\")(x)\n\n    x = layers.Activation(\"relu\", name=\"block13_sepconv2_act\")(x)\n    x = layers.SeparableConv2D(1024, (3, 3), padding=\"same\", use_bias=False, name=\"block13_sepconv2\")(x)\n    x = layers.BatchNormalization(name=\"block13_sepconv2_bn\")(x)\n\n    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\", name=\"block13_pool\")(x)\n    x = layers.Add(name=\"block13_add\")([x, residual])\n\n    # block14\n    x = layers.SeparableConv2D(1536, (3, 3), padding=\"same\", use_bias=False, name=\"block14_sepconv1\")(x)\n    x = layers.BatchNormalization(name=\"block14_sepconv1_bn\")(x)\n    x = layers.Activation(\"relu\", name=\"block14_sepconv1_act\")(x)\n\n    x = layers.SeparableConv2D(2048, (3, 3), padding=\"same\", use_bias=False, name=\"block14_sepconv2\")(x)\n    x = layers.BatchNormalization(name=\"block14_sepconv2_bn\")(x)\n    x = layers.Activation(\"relu\", name=\"block14_sepconv2_act\")(x)\n\n    return models.Model(img_input, x, name=\"xception_backbone_custom\")\n\n\ndef build_xception_binary(weights_path=None, input_shape=(299, 299, 3), dropout=0.5):\n    backbone = build_xception_backbone_exact(input_shape=input_shape)\n\n    if weights_path and os.path.exists(weights_path):\n        backbone.load_weights(weights_path)  # load NOTOP weights into backbone\n        print(\"[INFO] Loaded ImageNet NOTOP weights into backbone:\", weights_path)\n    else:\n        print(\"[INFO] ImageNet weights not provided or not found. Training from scratch.\")\n\n    x = backbone.output\n    x = layers.GlobalAveragePooling2D(name=\"gap\")(x)\n    x = layers.Dropout(dropout, name=\"drop\")(x)\n    outputs = layers.Dense(1, activation=\"sigmoid\", name=\"pred\")(x)\n\n    model = models.Model(backbone.input, outputs, name=\"Xception_custom_bin\")\n\n    return model, backbone\n\n\n# -------------------------\n# Video utilities -> intervals\n# -------------------------\ndef smooth_predictions(predictions, window_size=7, threshold=0.5):\n    smoothed = []\n    preds = list(map(float, predictions))\n    for i in range(len(preds)):\n        w = preds[max(0, i - window_size // 2): i + window_size // 2 + 1]\n        smoothed.append(1 if float(np.mean(w)) >= threshold else 0)\n    return smoothed\n\ndef to_intervals(times, labels01, min_len_sec=0.3):\n    intervals = []\n    start = None\n    for t, y in zip(times, labels01):\n        if y == 1 and start is None:\n            start = t\n        elif y == 0 and start is not None:\n            end = t\n            if end - start >= min_len_sec:\n                intervals.append((start, end))\n            start = None\n    if start is not None and times:\n        end = times[-1]\n        if end - start >= min_len_sec:\n            intervals.append((start, end))\n    return intervals\n\ndef process_video_batch(video_path, model, target_size, batch_size=32, skip_frames=3):\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        raise RuntimeError(\"[ERROR] Не вдалося відкрити відео!\")\n\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    frame_probs = []\n    frame_times = []\n\n    batch = []\n    batch_times = []\n    frame_idx = 0\n    t0 = time.time()\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame_idx += 1\n        if skip_frames > 1 and (frame_idx % skip_frames) != 0:\n            continue\n\n        t_sec = frame_idx / fps\n        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, target_size)\n\n        # preprocess to [-1, 1]\n        arr = img.astype(np.float32)\n        arr = (arr / 127.5) - 1.0\n\n        batch.append(arr)\n        batch_times.append(t_sec)\n\n        if len(batch) == batch_size:\n            p = model.predict(np.array(batch), verbose=0).reshape(-1)\n            frame_probs.extend(p.tolist())\n            frame_times.extend(batch_times)\n            batch, batch_times = [], []\n\n    if batch:\n        p = model.predict(np.array(batch), verbose=0).reshape(-1)\n        frame_probs.extend(p.tolist())\n        frame_times.extend(batch_times)\n\n    cap.release()\n    total_time = time.time() - t0\n    return frame_probs, frame_times, total_time\n\n\n# -------------------------\n# Main\n# -------------------------\ndef main():\n    print(\"[INFO] TensorFlow:\", tf.__version__)\n    print(\"[INFO] GPU:\", tf.config.list_physical_devices(\"GPU\"))\n\n    dataset_dir = CFG.DATASET_DIR or find_binary_dataset_dir()\n    if dataset_dir is None:\n        raise RuntimeError(\"Не знайдено dataset/{positive,negative} у /kaggle/input. Вкажіть CFG.DATASET_DIR вручну.\")\n    print(\"[INFO] Using DATASET_DIR:\", dataset_dir)\n\n    if os.path.exists(CFG.WORK_RAW_DIR):\n        shutil.rmtree(CFG.WORK_RAW_DIR)\n    shutil.copytree(dataset_dir, CFG.WORK_RAW_DIR)\n    dataset_dir = CFG.WORK_RAW_DIR\n\n    # clean corrupted\n    total, removed = clean_corrupted_images(dataset_dir)\n    print(f\"[INFO] Cleaned corrupted images: removed={removed} / scanned={total}\")\n\n    offline_augment_to_balance(\n        src_dir=dataset_dir,\n        out_dir=CFG.WORK_AUG_DIR,\n        target_size=CFG.TARGET_SIZE,\n        max_per_image=CFG.OFFLINE_AUG_MAX_PER_IMAGE\n    )\n\n    # build train/val/test split\n    make_splits(CFG.WORK_AUG_DIR, CFG.WORK_SPLIT_DIR, ratios=CFG.SPLIT_RATIOS, seed=SEED)\n\n    if CFG.ONLINE_AUG:\n        train_datagen = ImageDataGenerator(\n            preprocessing_function=xception_preprocess,\n            rotation_range=15,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            shear_range=0.1,\n            zoom_range=0.1,\n            horizontal_flip=True,\n            brightness_range=[0.9, 1.1],\n        )\n    else:\n        train_datagen = ImageDataGenerator(preprocessing_function=xception_preprocess)\n\n    eval_datagen = ImageDataGenerator(preprocessing_function=xception_preprocess)\n\n    train_gen = train_datagen.flow_from_directory(\n        os.path.join(CFG.WORK_SPLIT_DIR, \"train\"),\n        target_size=CFG.TARGET_SIZE,\n        batch_size=CFG.BATCH_SIZE,\n        class_mode=\"binary\",\n        shuffle=True,\n        seed=SEED\n    )\n    val_gen = eval_datagen.flow_from_directory(\n        os.path.join(CFG.WORK_SPLIT_DIR, \"val\"),\n        target_size=CFG.TARGET_SIZE,\n        batch_size=CFG.BATCH_SIZE,\n        class_mode=\"binary\",\n        shuffle=False\n    )\n    test_gen = eval_datagen.flow_from_directory(\n        os.path.join(CFG.WORK_SPLIT_DIR, \"test\"),\n        target_size=CFG.TARGET_SIZE,\n        batch_size=CFG.BATCH_SIZE,\n        class_mode=\"binary\",\n        shuffle=False\n    )\n\n    print(\"[INFO] class_indices:\", train_gen.class_indices)  # usually {'negative':0, 'positive':1}\n\n    class_weights = compute_class_weight(\n        class_weight=\"balanced\",\n        classes=np.unique(train_gen.classes),\n        y=train_gen.classes\n    )\n    class_weight_dict = dict(enumerate(class_weights))\n    print(\"[INFO] class_weight_dict:\", class_weight_dict)\n\n    model, backbone = build_xception_binary(\n        weights_path=CFG.IMAGENET_WEIGHTS_PATH,\n        input_shape=CFG.TARGET_SIZE + (3,),\n        dropout=0.5\n    )\n\n    best_weights_path = \"/kaggle/working/best_model.weights.h5\"\n\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor=\"val_loss\",\n            patience=CFG.PATIENCE,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor=\"val_loss\",\n            factor=0.5,\n            patience=max(2, CFG.PATIENCE // 2),\n            min_lr=1e-7,\n            verbose=1\n        ),\n        tf.keras.callbacks.ModelCheckpoint(\n            best_weights_path,\n            monitor=\"val_accuracy\",\n            save_best_only=True,\n            save_weights_only=True,\n            verbose=1\n        )\n    ]\n\n    history_all = {\"accuracy\": [], \"val_accuracy\": [], \"loss\": [], \"val_loss\": [], \"auc\": [], \"val_auc\": []}\n\n    using_pretrained = CFG.IMAGENET_WEIGHTS_PATH and os.path.exists(CFG.IMAGENET_WEIGHTS_PATH)\n    frozen_epochs = CFG.FROZEN_EPOCHS if using_pretrained else 0\n    fine_epochs = CFG.EPOCHS - frozen_epochs\n\n    if frozen_epochs > 0:\n        print(\"\\n\" + \"=\" * 60)\n        print(\"[INFO] PHASE 1: Train head only (backbone frozen)\")\n        print(\"=\" * 60 + \"\\n\")\n\n        backbone.trainable = False\n        model.compile(\n            optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.LR_FROZEN),\n            loss=\"binary_crossentropy\",\n            metrics=[\"accuracy\", tf.keras.metrics.AUC(name=\"auc\")]\n        )\n\n        hist1 = model.fit(\n            train_gen,\n            validation_data=val_gen,\n            epochs=frozen_epochs,\n            class_weight=class_weight_dict,\n            callbacks=callbacks,\n            verbose=1\n        )\n        for k in history_all:\n            if k in hist1.history:\n                history_all[k].extend(hist1.history[k])\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"[INFO] PHASE 2: Fine-tuning (backbone trainable)\")\n    print(\"=\" * 60 + \"\\n\")\n\n    backbone.trainable = True\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.LR_FINE),\n        loss=\"binary_crossentropy\",\n        metrics=[\"accuracy\", tf.keras.metrics.AUC(name=\"auc\")]\n    )\n\n    hist2 = model.fit(\n        train_gen,\n        validation_data=val_gen,\n        epochs=frozen_epochs + fine_epochs,\n        initial_epoch=frozen_epochs,\n        class_weight=class_weight_dict,\n        callbacks=callbacks,\n        verbose=1\n    )\n    for k in history_all:\n        if k in hist2.history:\n            history_all[k].extend(hist2.history[k])\n\n    model.save(\"/kaggle/working/xception_custom_binary.keras\")\n    model.load_weights(best_weights_path)\n\n    # Evaluate on test\n    print(\"[INFO] Evaluating on test split...\")\n    y_prob = model.predict(test_gen, verbose=1).reshape(-1)\n    y_pred = (y_prob >= 0.5).astype(int)\n    y_true = test_gen.classes\n\n    cm = confusion_matrix(y_true, y_pred)\n    tn, fp, fn, tp = cm.ravel()\n\n    accuracy = (tp + tn) / (tp + tn + fp + fn)\n    precision = tp / (tp + fp) if (tp + fp) else 0.0\n    recall = tp / (tp + fn) if (tp + fn) else 0.0\n    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0\n\n    print(\"\\n[INFO] Confusion matrix:\\n\", cm)\n    print(f\"[INFO] Accuracy : {accuracy:.4f}\")\n    print(f\"[INFO] Precision: {precision:.4f}\")\n    print(f\"[INFO] Recall   : {recall:.4f}\")\n    print(f\"[INFO] F1-score : {f1:.4f}\\n\")\n    print(classification_report(y_true, y_pred, target_names=[\"Negative\", \"Positive\"]))\n\n    plt.figure(figsize=(7, 6))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n                xticklabels=[\"Neg\", \"Pos\"], yticklabels=[\"Neg\", \"Pos\"])\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.title(\"Confusion Matrix\")\n    plt.tight_layout()\n    plt.savefig(\"/kaggle/working/confusion_matrix.png\", dpi=250)\n    plt.close()\n\n    # Training curves\n    plt.figure(figsize=(14, 4))\n    plt.subplot(1, 3, 1)\n    plt.plot(history_all.get(\"accuracy\", []), label=\"train\")\n    plt.plot(history_all.get(\"val_accuracy\", []), label=\"val\")\n    plt.title(\"Accuracy\"); plt.legend(); plt.grid(alpha=0.3)\n\n    plt.subplot(1, 3, 2)\n    plt.plot(history_all.get(\"loss\", []), label=\"train\")\n    plt.plot(history_all.get(\"val_loss\", []), label=\"val\")\n    plt.title(\"Loss\"); plt.legend(); plt.grid(alpha=0.3)\n\n    plt.subplot(1, 3, 3)\n    plt.plot(history_all.get(\"auc\", []), label=\"train\")\n    plt.plot(history_all.get(\"val_auc\", []), label=\"val\")\n    plt.title(\"AUC\"); plt.legend(); plt.grid(alpha=0.3)\n\n    plt.tight_layout()\n    plt.savefig(\"/kaggle/working/training_curves.png\", dpi=250)\n    plt.close()\n\n    # Video inference -> intervals\n    if CFG.VIDEO_PATH and os.path.exists(CFG.VIDEO_PATH):\n        print(\"[INFO] Video inference:\", CFG.VIDEO_PATH)\n        probs, times_sec, tproc = process_video_batch(\n            CFG.VIDEO_PATH, model, CFG.TARGET_SIZE,\n            batch_size=CFG.VIDEO_BATCH,\n            skip_frames=CFG.VIDEO_SKIP_FRAMES\n        )\n        labels = smooth_predictions(probs, window_size=CFG.SMOOTH_WINDOW, threshold=CFG.VIDEO_THRESHOLD)\n        intervals = to_intervals(times_sec, labels, min_len_sec=CFG.MIN_INTERVAL_SEC)\n\n        print(f\"[INFO] Video processed in {tproc:.2f}s, intervals found: {len(intervals)}\")\n        out_csv = \"/kaggle/working/video_intervals.csv\"\n        with open(out_csv, \"w\", encoding=\"utf-8\") as f:\n            f.write(\"start_sec,end_sec\\n\")\n            for a, b in intervals:\n                f.write(f\"{a:.3f},{b:.3f}\\n\")\n        print(\"[INFO] Saved intervals to:\", out_csv)\n\n        plt.figure(figsize=(14, 5))\n        plt.plot(times_sec, probs, label=\"prob\", alpha=0.7)\n        plt.axhline(CFG.VIDEO_THRESHOLD, color=\"red\", linestyle=\"--\", label=\"threshold\")\n        plt.title(\"Video predictions over time\")\n        plt.xlabel(\"time (sec)\")\n        plt.ylabel(\"P(positive)\")\n        plt.grid(alpha=0.3)\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(\"/kaggle/working/video_predictions.png\", dpi=250)\n        plt.close()\n    else:\n        print(\"[INFO] VIDEO_PATH not set or file not found. Skipping video step.\")\n\n    print(\"[INFO] Done. Outputs are in /kaggle/working/\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-17T23:11:24.686793Z","iopub.execute_input":"2025-12-17T23:11:24.687579Z","iopub.status.idle":"2025-12-17T23:27:45.159092Z","shell.execute_reply.started":"2025-12-17T23:11:24.687537Z","shell.execute_reply":"2025-12-17T23:27:45.158423Z"}},"outputs":[{"name":"stderr","text":"2025-12-17 23:11:27.526970: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766013087.703437      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766013087.761601      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766013088.204435      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766013088.204477      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766013088.204479      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766013088.204482      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"[INFO] TensorFlow: 2.19.0\n[INFO] GPU: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n[INFO] Using DATASET_DIR: /kaggle/input/lab6-spotify-dataset/dataset\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[INFO] Cleaned corrupted images: removed=0 / scanned=1368\n[INFO] Before offline augmentation: pos=671, neg=697\n[INFO] Offline augmenting minority='positive' to add ~26 images\n[INFO] After offline augmentation: pos=697, neg=697\n[INFO] Split 'positive': train=487, val=104, test=106\n[INFO] Split 'negative': train=487, val=104, test=106\nFound 974 images belonging to 2 classes.\nFound 208 images belonging to 2 classes.\nFound 212 images belonging to 2 classes.\n[INFO] class_indices: {'negative': 0, 'positive': 1}\n[INFO] class_weight_dict: {0: np.float64(1.0), 1: np.float64(1.0)}\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1766013163.048588      55 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"[INFO] Loaded ImageNet NOTOP weights into backbone: /kaggle/input/xception_weights_tf_dim_ordering_tf_kernels_notop/tensorflow2/default/1/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n\n============================================================\n[INFO] PHASE 1: Train head only (backbone frozen)\n============================================================\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1766013172.557988     143 service.cc:152] XLA service 0x787370002550 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1766013172.558027     143 service.cc:160]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1766013173.717930     143 cuda_dnn.cc:529] Loaded cuDNN version 91002\nI0000 00:00:1766013183.384256     143 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m24/61\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 465ms/step - accuracy: 0.5990 - auc: 0.6236 - loss: 0.6579","output_type":"stream"},{"name":"stderr","text":"2025-12-17 23:13:25.791580: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2025-12-17 23:13:26.028057: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2025-12-17 23:13:26.988077: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2025-12-17 23:13:27.247893: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719ms/step - accuracy: 0.7107 - auc: 0.7763 - loss: 0.5588\nEpoch 1: val_accuracy improved from -inf to 0.93269, saving model to /kaggle/working/best_model.weights.h5\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 947ms/step - accuracy: 0.7126 - auc: 0.7787 - loss: 0.5567 - val_accuracy: 0.9327 - val_auc: 0.9848 - val_loss: 0.2569 - learning_rate: 0.0010\nEpoch 2/5\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 454ms/step - accuracy: 0.9393 - auc: 0.9864 - loss: 0.2295\nEpoch 2: val_accuracy improved from 0.93269 to 0.94231, saving model to /kaggle/working/best_model.weights.h5\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 499ms/step - accuracy: 0.9394 - auc: 0.9864 - loss: 0.2291 - val_accuracy: 0.9423 - val_auc: 0.9887 - val_loss: 0.1956 - learning_rate: 0.0010\nEpoch 3/5\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 463ms/step - accuracy: 0.9489 - auc: 0.9874 - loss: 0.1802\nEpoch 3: val_accuracy did not improve from 0.94231\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 504ms/step - accuracy: 0.9490 - auc: 0.9875 - loss: 0.1800 - val_accuracy: 0.9375 - val_auc: 0.9905 - val_loss: 0.1627 - learning_rate: 0.0010\nEpoch 4/5\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 453ms/step - accuracy: 0.9547 - auc: 0.9906 - loss: 0.1426\nEpoch 4: val_accuracy improved from 0.94231 to 0.95192, saving model to /kaggle/working/best_model.weights.h5\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 499ms/step - accuracy: 0.9546 - auc: 0.9906 - loss: 0.1426 - val_accuracy: 0.9519 - val_auc: 0.9915 - val_loss: 0.1485 - learning_rate: 0.0010\nEpoch 5/5\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 451ms/step - accuracy: 0.9684 - auc: 0.9922 - loss: 0.1271\nEpoch 5: val_accuracy did not improve from 0.95192\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 491ms/step - accuracy: 0.9683 - auc: 0.9922 - loss: 0.1271 - val_accuracy: 0.9471 - val_auc: 0.9921 - val_loss: 0.1343 - learning_rate: 0.0010\nRestoring model weights from the end of the best epoch: 5.\n\n============================================================\n[INFO] PHASE 2: Fine-tuning (backbone trainable)\n============================================================\n\nEpoch 6/30\n","output_type":"stream"},{"name":"stderr","text":"2025-12-17 23:16:43.791867: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2025-12-17 23:16:44.000416: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2025-12-17 23:16:45.057969: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2025-12-17 23:16:45.286263: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2025-12-17 23:16:46.197678: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2025-12-17 23:16:46.386460: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2025-12-17 23:16:47.297231: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2025-12-17 23:16:47.495808: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2025-12-17 23:16:47.888478: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2025-12-17 23:16:48.111673: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m58/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 469ms/step - accuracy: 0.9052 - auc: 0.9741 - loss: 0.2790","output_type":"stream"},{"name":"stderr","text":"2025-12-17 23:17:45.058634: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2025-12-17 23:17:45.294270: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2025-12-17 23:17:48.688522: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2025-12-17 23:17:48.893767: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2025-12-17 23:17:49.931229: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2025-12-17 23:17:50.157566: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2025-12-17 23:17:51.020750: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2025-12-17 23:17:51.208639: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2025-12-17 23:17:52.078623: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2025-12-17 23:17:52.277135: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2025-12-17 23:17:52.668698: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2025-12-17 23:17:52.891559: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9073 - auc: 0.9746 - loss: 0.2734 \nEpoch 6: val_accuracy improved from 0.95192 to 0.96154, saving model to /kaggle/working/best_model.weights.h5\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - accuracy: 0.9079 - auc: 0.9748 - loss: 0.2717 - val_accuracy: 0.9615 - val_auc: 0.9982 - val_loss: 0.1165 - learning_rate: 1.0000e-04\nEpoch 7/30\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 483ms/step - accuracy: 0.9939 - auc: 0.9994 - loss: 0.0445\nEpoch 7: val_accuracy improved from 0.96154 to 0.97115, saving model to /kaggle/working/best_model.weights.h5\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 536ms/step - accuracy: 0.9939 - auc: 0.9994 - loss: 0.0444 - val_accuracy: 0.9712 - val_auc: 0.9994 - val_loss: 0.0482 - learning_rate: 1.0000e-04\nEpoch 8/30\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 480ms/step - accuracy: 0.9981 - auc: 1.0000 - loss: 0.0106\nEpoch 8: val_accuracy improved from 0.97115 to 0.97596, saving model to /kaggle/working/best_model.weights.h5\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 535ms/step - accuracy: 0.9981 - auc: 1.0000 - loss: 0.0106 - val_accuracy: 0.9760 - val_auc: 0.9995 - val_loss: 0.0409 - learning_rate: 1.0000e-04\nEpoch 9/30\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 481ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0083\nEpoch 9: val_accuracy improved from 0.97596 to 0.98077, saving model to /kaggle/working/best_model.weights.h5\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 535ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0082 - val_accuracy: 0.9808 - val_auc: 0.9995 - val_loss: 0.0339 - learning_rate: 1.0000e-04\nEpoch 10/30\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 485ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0069\nEpoch 10: val_accuracy did not improve from 0.98077\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 522ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0069 - val_accuracy: 0.9760 - val_auc: 0.9995 - val_loss: 0.0414 - learning_rate: 1.0000e-04\nEpoch 11/30\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 479ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0025\nEpoch 11: val_accuracy did not improve from 0.98077\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 518ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0025 - val_accuracy: 0.9808 - val_auc: 0.9996 - val_loss: 0.0268 - learning_rate: 1.0000e-04\nEpoch 12/30\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 476ms/step - accuracy: 0.9982 - auc: 0.9988 - loss: 0.0143\nEpoch 12: val_accuracy improved from 0.98077 to 0.98558, saving model to /kaggle/working/best_model.weights.h5\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 527ms/step - accuracy: 0.9982 - auc: 0.9988 - loss: 0.0143 - val_accuracy: 0.9856 - val_auc: 0.9991 - val_loss: 0.0380 - learning_rate: 1.0000e-04\nEpoch 13/30\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 490ms/step - accuracy: 0.9955 - auc: 1.0000 - loss: 0.0070\nEpoch 13: val_accuracy did not improve from 0.98558\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 528ms/step - accuracy: 0.9955 - auc: 1.0000 - loss: 0.0070 - val_accuracy: 0.9856 - val_auc: 0.9996 - val_loss: 0.0334 - learning_rate: 1.0000e-04\nEpoch 14/30\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 483ms/step - accuracy: 0.9989 - auc: 1.0000 - loss: 0.0078\nEpoch 14: val_accuracy improved from 0.98558 to 0.99519, saving model to /kaggle/working/best_model.weights.h5\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 539ms/step - accuracy: 0.9989 - auc: 1.0000 - loss: 0.0078 - val_accuracy: 0.9952 - val_auc: 0.9993 - val_loss: 0.0259 - learning_rate: 1.0000e-04\nEpoch 15/30\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 493ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0035\nEpoch 15: val_accuracy did not improve from 0.99519\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 531ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0034 - val_accuracy: 0.9952 - val_auc: 0.9994 - val_loss: 0.0243 - learning_rate: 1.0000e-04\nEpoch 16/30\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 475ms/step - accuracy: 0.9984 - auc: 1.0000 - loss: 0.0026\nEpoch 16: val_accuracy did not improve from 0.99519\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 512ms/step - accuracy: 0.9984 - auc: 1.0000 - loss: 0.0026 - val_accuracy: 0.9952 - val_auc: 0.9950 - val_loss: 0.0292 - learning_rate: 1.0000e-04\nEpoch 17/30\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 8.6328e-04\nEpoch 17: val_accuracy did not improve from 0.99519\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 508ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 8.6447e-04 - val_accuracy: 0.9952 - val_auc: 0.9950 - val_loss: 0.0293 - learning_rate: 1.0000e-04\nEpoch 18/30\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 482ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.4280e-04\nEpoch 18: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n\nEpoch 18: val_accuracy did not improve from 0.99519\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 519ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 6.4398e-04 - val_accuracy: 0.9952 - val_auc: 0.9950 - val_loss: 0.0310 - learning_rate: 1.0000e-04\nEpoch 19/30\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471ms/step - accuracy: 0.9997 - auc: 1.0000 - loss: 0.0018\nEpoch 19: val_accuracy did not improve from 0.99519\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 508ms/step - accuracy: 0.9997 - auc: 1.0000 - loss: 0.0019 - val_accuracy: 0.9952 - val_auc: 0.9950 - val_loss: 0.0378 - learning_rate: 5.0000e-05\nEpoch 20/30\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 478ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0010\nEpoch 20: val_accuracy did not improve from 0.99519\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 515ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0010 - val_accuracy: 0.9952 - val_auc: 0.9950 - val_loss: 0.0326 - learning_rate: 5.0000e-05\nEpoch 21/30\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 489ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 8.4989e-04\nEpoch 21: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n\nEpoch 21: val_accuracy did not improve from 0.99519\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 526ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 8.4566e-04 - val_accuracy: 0.9952 - val_auc: 0.9996 - val_loss: 0.0288 - learning_rate: 5.0000e-05\nEpoch 21: early stopping\nRestoring model weights from the end of the best epoch: 15.\n[INFO] Evaluating on test split...\n\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step \n\n[INFO] Confusion matrix:\n [[105   1]\n [  2 104]]\n[INFO] Accuracy : 0.9858\n[INFO] Precision: 0.9905\n[INFO] Recall   : 0.9811\n[INFO] F1-score : 0.9858\n\n              precision    recall  f1-score   support\n\n    Negative       0.98      0.99      0.99       106\n    Positive       0.99      0.98      0.99       106\n\n    accuracy                           0.99       212\n   macro avg       0.99      0.99      0.99       212\nweighted avg       0.99      0.99      0.99       212\n\n[INFO] Video inference: /kaggle/input/spoti-detection/spotify-video-for-detection.mp4\n","output_type":"stream"},{"name":"stderr","text":"2025-12-17 23:27:31.262821: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2025-12-17 23:27:31.503960: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2025-12-17 23:27:32.601009: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n2025-12-17 23:27:32.866465: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n","output_type":"stream"},{"name":"stdout","text":"[INFO] Video processed in 40.65s, intervals found: 2\n[INFO] Saved intervals to: /kaggle/working/video_intervals.csv\n[INFO] Done. Outputs are in /kaggle/working/\n","output_type":"stream"}],"execution_count":1}]}